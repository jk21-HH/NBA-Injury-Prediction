{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_scrape(year):\n",
    "    url = \"https://www.basketball-reference.com/leagues/NBA_{}_per_game.html\".format(year)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html,features=\"html\")\n",
    "    soup.findAll('tr', limit=1)\n",
    "    headers = [th.getText() for th in soup.findAll('tr', limit=1)[0].findAll('th')]\n",
    "    headers = headers[1:]\n",
    "    rows = soup.findAll('tr')[1:]\n",
    "    player_stats = [[td.getText() for td in rows[i].findAll('td')] for i in range(len(rows))]\n",
    "    player_stats_df = pd.DataFrame(player_stats, columns = headers)\n",
    "    player_stats_df.dropna(subset = ['Player'], inplace = True)\n",
    "    player_stats_df.insert(0, \"Year\", [year - 1]*(len(player_stats_df.index)))\n",
    "    return player_stats_df\n",
    "\n",
    "year_list = list(range(2011,2021))\n",
    "all_player_stats_df = pd.DataFrame()\n",
    "for year in year_list:\n",
    "    reg_season_df = stats_scrape(year)\n",
    "    all_player_stats_df=pd.concat([all_player_stats_df,reg_season_df], ignore_index=True)\n",
    "    time.sleep(2)\n",
    "all_player_stats_df.to_csv('C:\\prject\\player_stats_2010_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_player_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_list = range(2010,2020)\n",
    "team_short_name=['ATL','BOS','BRK','CHA','CHI','CHO','CLE','DAL','DEN','DET','GSW','HOU','IND','LAC','LAL','MEM',\n",
    "       'MIA','MIL','MIN','NJN','NOH','NOP','NYK','OKC','ORL','PHI','PHO','POR','SAC','SAS','TOR','UTA','WAS']\n",
    "teams_relocate_rename_dict = {\n",
    "    'NJN': ['2010', '2011', '2012'],\n",
    "    'BRK': ['2013', '2014', '2015', '2016', '2017', '2018', '2019','2020'],\n",
    "    'CHA': ['2010', '2011', '2012', '2013','2014'],\n",
    "    'CHO': ['2015', '2016', '2017', '2018', '2019','2020'],\n",
    "    'NOH': ['2010','2011','2012','2013'],\n",
    "    'NOP': ['2014', '2015', '2016', '2017', '2018', '2019','2020'],\n",
    "}\n",
    "\n",
    "def sched_scrape_cleaning(team_abrv,year):\n",
    "    \n",
    "    url = \"https://www.basketball-reference.com/teams/{}/{}_games.html\". format(team_abrv,year)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html,features=\"html\")\n",
    "    soup.findAll('tr', limit=1)\n",
    "    rows = soup.findAll('tr')\n",
    "    sched_data = [[td.getText() for td in rows[i].findAll('td')] for i in range(len(rows))]\n",
    "    df = pd.DataFrame(sched_data)\n",
    "    df.drop(columns = [1,2,3,6,8,9,10,11,12,13], inplace = True)\n",
    "    df.columns = ['Date','Away_flag','Opponent','OT_flag']\n",
    "    df.dropna(subset = ['Date'], inplace = True)\n",
    "    df.reset_index(inplace = True)\n",
    "    df['Game_num'] = df.index + 1\n",
    "    TeamName=soup.findAll(\"span\",itemprop=\"name\")[2]\n",
    "    df['Team'] = TeamName.text\n",
    "    df['Year'] = str(int(year) - 1)\n",
    "    df = df[['Team','Year', 'Game_num','Date','Away_flag','Opponent','OT_flag']]\n",
    "    return df\n",
    "\n",
    "all_teams_sched_df = pd.DataFrame()\n",
    "for team in team_short_name:\n",
    "    team_sched_df = pd.DataFrame(columns = ['Team','Year','Game_num','Date','Away_flag','Opponent','OT_flag'])\n",
    "    if team not in teams_relocate_rename_dict:\n",
    "        for year in season_list:\n",
    "            single_season_df = sched_scrape_cleaning(team, year)\n",
    "            team_sched_df=pd.concat([team_sched_df,single_season_df], ignore_index=True)\n",
    "            time.sleep(2)\n",
    "    else:    \n",
    "        for year in teams_relocate_rename_dict[team]:\n",
    "            single_season_df = sched_scrape_cleaning(team, year)\n",
    "            team_sched_df=pd.concat([team_sched_df,single_season_df], ignore_index=True)\n",
    "            time.sleep(2)\n",
    "    all_teams_sched_df = pd.concat([all_teams_sched_df, team_sched_df], ignore_index=True)   \n",
    "all_teams_sched_df.to_csv(r'C:\\prject\\all_teams_schedule_2010_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_teams_sched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.prosportstransactions.com/basketball/Search/SearchResults.php?Player=&Team=&BeginDate=2010-08-01&EndDate=2020-08-27&InjuriesChkBx=yes&PersonalChkBx=yes&Submit=Search\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "df_first_page = pd.read_html(url)\n",
    "df_first_page = df_first_page[0]\n",
    "df_first_page.drop([0], inplace = True)\n",
    "df_first_page[3]=df_first_page[3].str[2:]\n",
    "df_first_page.columns = ['Date','Team','Acquired','Player','Notes']\n",
    "df_first_page.drop(['Acquired'],axis=1,inplace=True)\n",
    "appended_data = df_first_page\n",
    "for i in range(4,len(soup.findAll('a'))-4):\n",
    "    one_a_tag = soup.findAll('a')[i]\n",
    "    link = one_a_tag['href']\n",
    "    download_url = 'https://www.prosportstransactions.com/basketball/Search/'+ link\n",
    "    dfs = pd.read_html(download_url)\n",
    "    df = dfs[0]\n",
    "    df.drop([0], inplace = True)\n",
    "    df[3]=df[3].str[2:]\n",
    "    df.columns = ['Date','Team','Acquired','Player','Notes']\n",
    "    df.drop(['Acquired'],axis=1,inplace=True)\n",
    "    appended_data=pd.concat([appended_data,df], ignore_index=True)\n",
    "    time.sleep(3)\n",
    "appended_data.to_csv(r'C:\\prject\\prosportstransactions_scrape_missedgames_2010_2020.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
